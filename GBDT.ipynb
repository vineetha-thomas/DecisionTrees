{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "import numpy as np\n",
    "#from numba import jit\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: loss of least square regression and binary logistic regression\n",
    "'''\n",
    "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
    "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "'''\n",
    "class leastsquare(object):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,score):\n",
    "        return score\n",
    "\n",
    "    def g(self,true,score):\n",
    "        return -2*(true-score) \n",
    "\n",
    "    def h(self,true,score):\n",
    "        return ([2]*len(score))\n",
    "    \n",
    "    def loss(self,true,score):\n",
    "        return np.sum((true-score)**2)\n",
    "\n",
    "class logistic(object):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    def pred(self,score):\n",
    "        prob = (1/(1+np.exp(-1*score)))\n",
    "        pred = np.array([1 if p > 0.5 else 0 for p in prob])\n",
    "        return pred        \n",
    "\n",
    "    def g(self,true,score):\n",
    "        y = true\n",
    "        yhat = score\n",
    "        return (-y/(1+np.exp(yhat))) + ((1-y)/(1+np.exp(-1*yhat)))\n",
    "\n",
    "    def h(self,true,score):\n",
    "        y = true\n",
    "        yhat = score\n",
    "        return (y*np.exp(yhat))/((1+np.exp(yhat))**2)  +  ((1-y)*np.exp(-yhat))/((1+np.exp(-yhat))**2)\n",
    "\n",
    "    def loss(self,true,score):\n",
    "        y = true\n",
    "        yhat = score\n",
    "        return np.sum(y*np.log(1+np.exp(-yhat)) + (1-y)*np.log(1+np.exp(yhat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of Random Forest\n",
    "class RF(object):\n",
    "    '''\n",
    "    Class of Random Forest\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth d_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        rf = 0.99, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        \n",
    "        #Instantiate loss class\n",
    "        if(self.loss == 'mse'):\n",
    "            self.lossType = leastsquare()\n",
    "        elif(self.loss == 'log'):\n",
    "            self.lossType = logistic()        \n",
    "        else:\n",
    "            print(\"Invalid choice of loss\")\n",
    "        \n",
    "        \n",
    "        self.trees = []\n",
    "        self.yhat0 = 0\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "        \n",
    "        print(\"INSIDE RF FIT(\" +\n",
    "              \"max_depth=\"+str(self.max_depth) +\n",
    "              \", min_sample_split=\"+str(self.min_sample_split) +\n",
    "              \", lamda=\"+str(self.lamda) +\n",
    "              \", gamma=\"+str(self.gamma) +           \n",
    "              \", rf=\"+str(self.rf) +\n",
    "              \", num_trees=\"+str(self.num_trees) +\n",
    "              \")\")\n",
    "        \n",
    "            \n",
    "        #Initialize yhat0\n",
    "        #self.yhat0 = np.mean(target)\n",
    "        #yhat0 = np.array([np.mean(target)]*len(target))\n",
    "        \n",
    "        self.yhat0 = 0\n",
    "        yhat0 = np.zeros(len(target))\n",
    "               \n",
    "        for k in range(self.num_trees):\n",
    "            #Random selection from train_target\n",
    "            #randIndex = np.random.choice(range(len(target)), len(target))\n",
    "            #X = train[randIndex,:]\n",
    "            #y = target[randIndex]\n",
    "            \n",
    "            X=train\n",
    "            y=target\n",
    "            \n",
    "           \n",
    "            #Calculate g and h\n",
    "            g = self.lossType.g(y, yhat0)\n",
    "            h = self.lossType.h(y, yhat0)\n",
    "            \n",
    "            #Instantiate the Tree object\n",
    "            myTree = Tree(n_threads = None, \n",
    "                      max_depth = self.max_depth, \n",
    "                      min_sample_split = self.min_sample_split,\n",
    "                      lamda = self.lamda,\n",
    "                      gamma = self.gamma, \n",
    "                      rf = self.rf)            \n",
    "                    \n",
    "            #Fit the kth tree\n",
    "            myTree = myTree.fit(X,g,h)\n",
    "            self.trees.append(myTree)\n",
    "            \n",
    "            if(DEBUG_FLAG1):\n",
    "                #Do the prediction to compute loss\n",
    "                yhat_k = self.predict(train)\n",
    "                \n",
    "                if(self.loss == 'mse'):\n",
    "                    loss = root_mean_square_error(target,self.lossType.pred(yhat_k))\n",
    "                    print(\"------Tree\"+str(k)+\": rmse loss=\" + str(loss)+\"-------\")\n",
    "\n",
    "                if(self.loss == 'log'):\n",
    "                    acc = accuracy(target,yhat_k)\n",
    "                    loss = self.lossType.loss(target,yhat_k)\n",
    "                    print(\"------Tree\"+str(k)+\": accuracy=\" + str(acc)+ \", loss=\" + str(loss) +\"-------\")   \n",
    "   \n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "               \n",
    "        yhat_k=0\n",
    "        for k in range(len(self.trees)):\n",
    "            yhat_k += self.trees[k].predict(test)\n",
    "            \n",
    "        \n",
    "        score = yhat_k/len(self.trees)  \n",
    "        score += np.array([self.yhat0]*test.shape[0]) #Add the bias term yhat_0\n",
    "        return self.lossType.pred(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of GBDT\n",
    "class GBDT(object):\n",
    "    '''\n",
    "    Class of gradient boosting decision tree (GBDT)\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth D_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        learning_rate: The learning rate eta of GBDT.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        learning_rate = 0.1, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_trees = num_trees\n",
    "        \n",
    "        self.trees = []\n",
    "        self.yhat0 = []\n",
    "\n",
    "\n",
    "       #Instantiate the loss class\n",
    "        if(self.loss == 'mse'):\n",
    "            self.lossType = leastsquare()\n",
    "        elif(self.loss == 'log'):\n",
    "            self.lossType = logistic()        \n",
    "        else:\n",
    "            print(\"Invalid choice of loss\")\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "        \n",
    "        print(\"INSIDE GBDT FIT(\" +\n",
    "              \", max_depth=\"+str(self.max_depth) +\n",
    "              \", min_sample_split=\"+str(self.min_sample_split) +\n",
    "              \", lamda=\"+str(self.lamda) +\n",
    "              \", gamma=\"+str(self.gamma) +           \n",
    "              \", lr=\"+str(self.learning_rate) +\n",
    "              \", num_trees=\"+str(self.num_trees) +\n",
    "              \")\")        \n",
    "            \n",
    "        \n",
    "        #Initialize yhat_prev\n",
    "        yhat_prev = np.array([np.mean(target)]*len(target))\n",
    "        self.yhat0 = np.mean(target)\n",
    "        \n",
    "        #yhat_prev = np.zeros(len(target))\n",
    "        #self.yhat0 = 0.0\n",
    "        \n",
    "        for k in range(self.num_trees):           \n",
    "            #Calculate g           \n",
    "            g = self.lossType.g(target,yhat_prev)\n",
    "            \n",
    "            #Calculate h\n",
    "            h = self.lossType.h(target,yhat_prev)            \n",
    "            \n",
    "            #Instantiate the Tree object\n",
    "            myTree = Tree(n_threads = None, \n",
    "                      max_depth = self.max_depth, \n",
    "                      min_sample_split = self.min_sample_split,\n",
    "                      lamda = self.lamda,\n",
    "                      gamma = self.gamma, \n",
    "                      rf = 0)\n",
    "            \n",
    "                \n",
    "            myTree = myTree.fit(train,g,h)\n",
    "            self.trees.append(myTree)            \n",
    "            \n",
    "            #Get prediction fk\n",
    "            fk = myTree.predict(train)\n",
    "            \n",
    "            #yhat_k = yhat_prev + eta*fk\n",
    "            yhat_k = yhat_prev + self.learning_rate*fk\n",
    "            \n",
    "            #yhat_prev = yhat_k\n",
    "            yhat_prev = yhat_k\n",
    "            \n",
    "            if(DEBUG_FLAG1):\n",
    "                #loss = lossType.loss(target,yhat_k)\n",
    "                if(self.loss == 'mse'):\n",
    "                    loss = root_mean_square_error(target,self.lossType.pred(yhat_k))\n",
    "                    print(\"------Tree\"+str(k)+\": rmse loss=\" + str(loss)+\"-------\")\n",
    "\n",
    "                if(self.loss == 'log'):\n",
    "                    acc = accuracy(target,self.lossType.pred(yhat_k))\n",
    "                    loss = self.lossType.loss(target,yhat_k)\n",
    "                    print(\"------Tree\"+str(k)+\": accuracy=\" + str(acc)+ \", loss=\" + str(loss) +\"-------\")   \n",
    "\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "        \n",
    "        yhat_prev = np.array([self.yhat0]*test.shape[0])\n",
    "        \n",
    "        for k in range(self.num_trees):\n",
    "            #Get the prediction from current tree\n",
    "            fk = self.trees[k].predict(test)  \n",
    "            yhat_k = yhat_prev + self.learning_rate*fk\n",
    "            yhat_prev = yhat_k\n",
    "            \n",
    "        \n",
    "        score = yhat_k\n",
    "        return self.lossType.pred(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of a node on a tree\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that are used for storing a node on a tree.\n",
    "    \n",
    "    A tree is presented by a set of nested TreeNodes,\n",
    "    with one TreeNode pointing two child TreeNodes,\n",
    "    until a tree leaf is reached.\n",
    "    \n",
    "    A node on a tree can be either a leaf node or a non-leaf node.\n",
    "    '''\n",
    "    \n",
    "    #TODO\n",
    "    def __init__(self, split_feature=0, split_threshold=0, left_child=None, right_child=None, is_leaf=False):\n",
    "        \n",
    "        self.is_leaf = is_leaf\n",
    "        self.split_threshold = split_threshold\n",
    "        self.split_feature = split_feature\n",
    "        \n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "        self.w = 0   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x[self.split_feature] < self.split_threshold:\n",
    "            return self.left\n",
    "        else:\n",
    "            return self.right\n",
    "        \n",
    "    def print(self):\n",
    "        print(\"Is leaf node =\", self.is_leaf)\n",
    "        print(\"Split_feature = \", self.split_feature)\n",
    "        print(\"Split_threshold = \", self.split_threshold)\n",
    "        print(\"Left Child = \", self.left)\n",
    "        print(\"Right Child = \", self.right)\n",
    "        print(\"w = \", self.w)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of single tree\n",
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
    "            rf = 0 means we are training a GBDT.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_threads = None, \n",
    "                 max_depth = 3, min_sample_split = 10,\n",
    "                 lamda = 1, gamma = 0, rf = 0):\n",
    "        self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = 0\n",
    "        self.int_member = 0\n",
    "        \n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, train, g, h):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        #TODO\n",
    "        \n",
    "        self.root = self.construct_tree(train,g,h,depth=0)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self,test):\n",
    "        '''\n",
    "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
    "        Return predictions (scores) as an array.\n",
    "        '''\n",
    "        #TODO\n",
    "        result = np.zeros(test.shape[0])\n",
    "        c = 0\n",
    "        \n",
    "        for x in test: #iterate through each row in test data matrix \n",
    "            curNode = self.root\n",
    "            while(curNode.is_leaf == False):\n",
    "                curNode = curNode.forward(x)\n",
    "            \n",
    "            result[c] = curNode.w\n",
    "            c +=1\n",
    " \n",
    "        return result\n",
    "\n",
    "    def construct_tree(self, train, g, h, depth):\n",
    "        '''\n",
    "        Tree construction, which is recursively used to grow a tree.\n",
    "        First we should check if we should stop further splitting.\n",
    "        \n",
    "        The stopping conditions include:\n",
    "            1. tree reaches max_depth $d_{max}$\n",
    "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
    "            3. gain <= 0\n",
    "        '''\n",
    "        #TODO\n",
    "\n",
    "        \n",
    "        #Check stopping critera\n",
    "        if(depth==self.max_depth):\n",
    "            #Max depth reached, this should be a leaf node\n",
    "            node = TreeNode(split_feature = 0, split_threshold=0, left_child=None, right_child=None, is_leaf=True)          \n",
    "            #Calculate the weight w of the leaf node\n",
    "            node.w = (-1*np.sum(g))/(np.sum(h)+self.lamda)  \n",
    "            if(DEBUG_FLAG2):\n",
    "                print(\"Leaf Node at max depth\")\n",
    "                #print(\"g=\",g)\n",
    "                #print(\"h=\",h)\n",
    "                print(\"w=\",node.w)\n",
    "            return node\n",
    "        \n",
    "        if(len(g)<self.min_sample_split):\n",
    "            #Less than min number of samples to split, this should be a leaf node\n",
    "            node = TreeNode(split_feature = 0, split_threshold=0, left_child=None, right_child=None, is_leaf=True) \n",
    " \n",
    "            #Calculate the weight w of the leaf node\n",
    "            node.w = (-1*np.sum(g))/(np.sum(h)+self.lamda)\n",
    "            if(DEBUG_FLAG2):\n",
    "                print(\"Leaf Node at min sample\")\n",
    "                #print(\"g=\",g)\n",
    "                #print(\"h=\",h)\n",
    "                print(\"w=\",node.w)\n",
    "            return node            \n",
    "        \n",
    "        \n",
    "        #Find the decision rule for train,g,h\n",
    "        p,t,gain = self.find_best_decision_rule(train,g,h)\n",
    "        if(DEBUG_FLAG2):\n",
    "            print(\"p=\"+str(p)+\", t=\"+str(t)+\", gain=\"+str(gain))\n",
    "        \n",
    "        \n",
    "        #Check the other stopping criteria\n",
    "        if(gain<=0):\n",
    "            #Dont split, thsi should be a leaf node\n",
    "            node = TreeNode(split_feature = 0, split_threshold=0, left_child=None, right_child=None, is_leaf=True)\n",
    "\n",
    "            #Calculate the weight w of the leaf node\n",
    "            node.w = (-1*np.sum(g))/(np.sum(h)+self.lamda)\n",
    "            if(DEBUG_FLAG2):\n",
    "                print(\"Leaf Node at min gain\")\n",
    "                #print(\"g=\",g)\n",
    "                #print(\"h=\",h)\n",
    "                print(\"w=\",node.w)\n",
    "            return node\n",
    "        \n",
    "        X1,g1,h1,X2,g2,h2 = self.split(train,g,h,p,t)\n",
    "        \n",
    "        node = TreeNode(split_feature=p, split_threshold=t, left_child=None, right_child=None, is_leaf=False)\n",
    "        node.left = self.construct_tree(X1,g1,h1,depth+1)\n",
    "        node.right = self.construct_tree(X2,g2,h2,depth+1)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def split(self,X,g,h,p,t):\n",
    "        \n",
    "        #Sort Xgh according to pth column\n",
    "        Xgh = np.append(X,np.array([g]).T, axis=1)\n",
    "        Xgh = np.append(Xgh,np.array([h]).T, axis=1)\n",
    "        Xgh = Xgh[np.argsort(Xgh[:,p])]\n",
    "    \n",
    "        X_sort = Xgh[:,0:X.shape[1]]\n",
    "        g_sort = Xgh[:,X.shape[1]]\n",
    "        h_sort = Xgh[:,Xgh.shape[1]-1]\n",
    "        \n",
    "        #Find the index to split according to threshold t\n",
    "        Xp = X_sort[:,p]\n",
    "        for i in range(len(Xp)):\n",
    "            if(Xp[i] < t):\n",
    "                i+=1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        X1 = X_sort[0:i,:]\n",
    "        g1 = g_sort[0:i]\n",
    "        h1 = h_sort[0:i]\n",
    "        \n",
    "        X2 = X_sort[i:X.shape[0],:]\n",
    "        g2 = g_sort[i:len(g)]\n",
    "        h2 = h_sort[i:len(h)]\n",
    "        \n",
    "        #X1 = X(pth column) < t\n",
    "        return X1,g1,h1,X2,g2,h2\n",
    "\n",
    "    def find_best_decision_rule(self, train, g, h):\n",
    "        '''\n",
    "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j, \n",
    "        train is the training data assigned to node j\n",
    "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
    "        g and h should be vectors of the same length as the number of data points in train\n",
    "        \n",
    "        for each feature, we find the best threshold by find_threshold(),\n",
    "        a [threshold, best_gain] list is returned for each feature.\n",
    "        Then we select the feature with the largest best_gain,\n",
    "        and return the best decision rule [feature, treshold] together with its gain.\n",
    "        '''\n",
    "        #TODO\n",
    "        best_gain = float('-inf')\n",
    "        best_feature = 0\n",
    "        best_threshold = 0\n",
    "        \n",
    "        #indices representing all feature indices\n",
    "        p_choices = list(range(0,train.shape[1]))\n",
    "        \n",
    "        if(self.rf==0): #GBDT\n",
    "            p_choices = p_choices\n",
    "        elif(self.rf > 1):\n",
    "            print(\"Wrong value for rf.\")\n",
    "        elif(self.rf<1): #Random  Forest\n",
    "            p_choices = random.sample(p_choices, round(rf*m))\n",
    "            \n",
    "            \n",
    "        for p in p_choices:\n",
    "            threshold,gain = self.find_threshold(g,h,train,p)\n",
    "            if (gain > best_gain):\n",
    "                best_gain = gain\n",
    "                best_threshold = threshold\n",
    "                best_feature = p\n",
    "                \n",
    "        \n",
    "        feature = best_feature\n",
    "        threshold = best_threshold\n",
    "        gain = best_gain\n",
    "        \n",
    "        return feature, threshold, gain\n",
    "    \n",
    "    def find_threshold(self, g, h, train,p):\n",
    "        '''\n",
    "        Given a particular feature $p_j$,\n",
    "        return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
    "        '''\n",
    "        #TODO\n",
    "        \n",
    "        #print(\"p=\",p)\n",
    "        \n",
    "        #Sort train,g,h according to 'p'\n",
    "        Xgh = np.append(train,np.array([g]).T, axis=1)\n",
    "        Xgh = np.append(Xgh,np.array([h]).T, axis=1)\n",
    "        Xgh = Xgh[np.argsort(Xgh[:,p])]\n",
    "        train_sort = Xgh[:,0:train.shape[1]]\n",
    "        g_sort = Xgh[:,train.shape[1]]\n",
    "        h_sort = Xgh[:,Xgh.shape[1]-1]\n",
    "        #print(\"g_sort=\",g_sort)\n",
    "        #print(\"h_sort=\",h_sort)\n",
    "        \n",
    "        #Come up with the list of thresholds\n",
    "        t_list = train_sort[:,p]\n",
    "        ts_list = np.roll(t_list, 1)\n",
    "        ts_list[0]= 0\n",
    "        t_list = 0.5*(t_list + ts_list)\n",
    "        t_list = t_list[1:len(t_list)] #The first entry is redundant actually.\n",
    "        #print(\"t_list=\",t_list)\n",
    "         \n",
    "        #Loop through the thresolds to find best gain\n",
    "        gain_best = float('-inf')\n",
    "        threshold_best = 0\n",
    "        for i in range(len(t_list)):\n",
    "            #print(\"g_sort[0:i+1]\",g_sort[0:i+1])\n",
    "            G_L = np.sum(g_sort[0:i+1])\n",
    "            G_R = np.sum(g_sort[i+1:len(g_sort)])\n",
    "            H_L = np.sum(h_sort[0:i+1])\n",
    "            H_R = np.sum(h_sort[i+1:len(h_sort)])  \n",
    "            #print(\"t=\",t_list[i])\n",
    "            #print(\"G_L=\", G_L)\n",
    "            #print(\"G_R=\", G_R)\n",
    "            #print(\"H_L=\", H_L)\n",
    "            #print(\"H_R=\", H_R)\n",
    "            gain = ((G_L*G_L)/(H_L+self.lamda)) \n",
    "            gain += ((G_R*G_R)/(H_R+self.lamda)) \n",
    "            gain -= (((G_L+G_R)**2)/(H_L+H_R+self.lamda))\n",
    "            #print(\"gain=\", gain)\n",
    "            gain = 0.5*gain - self.gamma\n",
    "\n",
    "            #gain = ((G_L*G_L)/(H_L+self.lamda)) + ((G_R*G_R)/(H_R+self.lamda)) - (((G_L+G_R)**2)/(H_L+H_R+self.lamda))\n",
    "            \n",
    "            \n",
    "            if(gain>gain_best):\n",
    "                gain_best = gain\n",
    "                threshold_best = t_list[i]\n",
    "                                           \n",
    "        threshold = threshold_best\n",
    "        best_gain = gain_best      \n",
    "        \n",
    "        return [threshold, best_gain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    #TODO\n",
    "    error = y - pred\n",
    "    rmse = np.sqrt(np.sum(np.dot(error,error))/len(error))\n",
    "    return rmse\n",
    "\n",
    "# precision\n",
    "def accuracy(pred, y):\n",
    "    #TODO\n",
    "    correct = 0\n",
    "    for j in range(len(y)):\n",
    "        if(pred[j]==y[j]):\n",
    "            correct +=1\n",
    "    precision = correct/len(pred) \n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSIDE GBDT FIT(, max_depth=4, min_sample_split=11, lamda=2, gamma=0.1, lr=0.3, num_trees=16)\n",
      "train_rmse= 1.6532897579764643\n",
      "test_rmse= 3.3151756403522863\n",
      "\n",
      "\n",
      "INSIDE RF FIT(max_depth=4, min_sample_split=11, lamda=2, gamma=0.1, rf=0.5, num_trees=16)\n",
      "train_rmse= 4.285572755247948\n",
      "test_rmse= 5.174724611481524\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEBUG_FLAG1 = False\n",
    "DEBUG_FLAG2 = False\n",
    "\n",
    "# TODO: GBDT regression on boston house price dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "for k in range(1):\n",
    "    max_depth1 = random.randint(2,10)\n",
    "    min_sample_split1 = random.randint(5,50)\n",
    "    lamda1 = random.uniform(0.001,10)\n",
    "    gamma1 = random.uniform(0,1)\n",
    "    learning_rate1 = random.uniform(0.1,1.0)\n",
    "    num_trees1 = random.randint(5,40)\n",
    "    \n",
    "    \n",
    "    max_depth1=4\n",
    "    min_sample_split1=11\n",
    "    lamda1=2\n",
    "    gamma1=0.1\n",
    "    lr1=0.3\n",
    "    num_trees1=16\n",
    "    \n",
    "    myGBDT = GBDT( n_threads = None, loss = 'mse',\n",
    "                  max_depth = max_depth1, min_sample_split=min_sample_split1, \n",
    "                  lamda=lamda1 , gamma=gamma1 ,\n",
    "                  learning_rate=lr1 , num_trees=num_trees1 )\n",
    "    \n",
    "    myGBDT = myGBDT.fit(X_train, y_train)\n",
    "    train_pred = myGBDT.predict(X_train)\n",
    "    test_pred = myGBDT.predict(X_test)\n",
    "    \n",
    "    train_rmse = root_mean_square_error(train_pred,y_train)\n",
    "    test_rmse = root_mean_square_error(test_pred,y_test)\n",
    "    print(\"train_rmse=\", train_rmse)\n",
    "    print(\"test_rmse=\", test_rmse)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "for k in range(1):\n",
    "    max_depth1 = random.randint(2,10)\n",
    "    min_sample_split1 = random.randint(5,50)\n",
    "    lamda1 = random.uniform(0.001,10)\n",
    "    gamma1 = random.uniform(0,1)\n",
    "    rf1 = random.uniform(0.2,0.5)\n",
    "    num_trees1 = random.randint(5,40)\n",
    "    \n",
    "    max_depth1=4\n",
    "    min_sample_split1=11\n",
    "    lamda1=2\n",
    "    gamma1=0.1\n",
    "    rf1=0.5\n",
    "    num_trees1=16\n",
    "    \n",
    "    myRF = RF( n_threads = None, loss = 'mse',\n",
    "              max_depth=max_depth1, min_sample_split=min_sample_split1, \n",
    "              lamda=lamda1, gamma=gamma1, rf=rf1,\n",
    "              num_trees=num_trees1)\n",
    "\n",
    "\n",
    "    myRF = myRF.fit(X_train, y_train)\n",
    "    train_pred = myRF.predict(X_train)\n",
    "    test_pred = myRF.predict(X_test)\n",
    "    train_rmse = root_mean_square_error(train_pred,y_train)\n",
    "    test_rmse = root_mean_square_error(test_pred,y_test)\n",
    "    print(\"train_rmse=\", train_rmse)\n",
    "    print(\"test_rmse=\", test_rmse)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSIDE GBDT FIT(, max_depth=4, min_sample_split=10, lamda=5.5, gamma=0.1, lr=0.7, num_trees=20)\n",
      "train_acc= 0.8071428571428572\n",
      "test_acc= 0.7733333333333333\n",
      "\n",
      "\n",
      "INSIDE RF FIT(max_depth=4, min_sample_split=10, lamda=5.5, gamma=0.1, rf=0.5, num_trees=20)\n",
      "train_acc= 0.7685714285714286\n",
      "test_acc= 0.7633333333333333\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on credit-g dataset\n",
    "\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "for k in range(1):\n",
    "    max_depth1 = random.randint(2,10)\n",
    "    min_sample_split1 = random.randint(5,50)\n",
    "    lamda1 = random.uniform(0.001,10)\n",
    "    gamma1 = random.uniform(0,1)\n",
    "    learning_rate1 = random.uniform(0.1,1.0)\n",
    "    num_trees1 = random.randint(5,40)\n",
    "    \n",
    "    \n",
    "    max_depth1=4\n",
    "    min_sample_split1=10\n",
    "    lamda1=5.5\n",
    "    gamma1=0.1\n",
    "    lr1=0.7\n",
    "    num_trees1=20\n",
    "    \n",
    "    myGBDT = GBDT( n_threads = None, loss = 'log',\n",
    "                  max_depth = max_depth1, min_sample_split=min_sample_split1, \n",
    "                  lamda=lamda1 , gamma=gamma1 ,\n",
    "                  learning_rate=lr1 , num_trees=num_trees1 )\n",
    "    \n",
    "    myGBDT = myGBDT.fit(X_train, y_train)\n",
    "    train_pred = myGBDT.predict(X_train)\n",
    "    test_pred = myGBDT.predict(X_test)\n",
    "    train_acc = accuracy(train_pred,y_train)\n",
    "    test_acc = accuracy(test_pred,y_test)\n",
    "    print(\"train_acc=\", train_acc)\n",
    "    print(\"test_acc=\", test_acc)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "for k in range(1):\n",
    "    max_depth1 = random.randint(2,10)\n",
    "    min_sample_split1 = random.randint(5,50)\n",
    "    lamda1 = random.uniform(0.001,10)\n",
    "    gamma1 = random.uniform(0,1)\n",
    "    rf1 = random.uniform(0.2,0.5)\n",
    "    num_trees1 = random.randint(5,40)\n",
    "    \n",
    "    max_depth1=4\n",
    "    min_sample_split1=10\n",
    "    lamda1=5.5\n",
    "    gamma1=0.1\n",
    "    rf1=0.5\n",
    "    num_trees1=20\n",
    "    \n",
    "    myRF = RF( n_threads = None, loss = 'log',\n",
    "              max_depth=max_depth1, min_sample_split=min_sample_split1, \n",
    "              lamda=lamda1, gamma=gamma1, rf=rf1,\n",
    "              num_trees=num_trees1)\n",
    "    \n",
    "    myRF = myRF.fit(X_train, y_train)\n",
    "    train_pred = myRF.predict(X_train)\n",
    "    test_pred = myRF.predict(X_test)\n",
    "    train_acc = accuracy(train_pred,y_train)\n",
    "    test_acc = accuracy(test_pred,y_test)\n",
    "    print(\"train_acc=\", train_acc)\n",
    "    print(\"test_acc=\", test_acc)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "max_depth1=2\n",
    "min_sample_split1=14\n",
    "lamda1=5.5\n",
    "gamma1=0.5\n",
    "lr1=0.4\n",
    "num_trees1=20\n",
    "myGBDT = GBDT( n_threads = None, loss = 'log',\n",
    "              max_depth = max_depth1, min_sample_split = min_sample_split1, \n",
    "              lamda = lamda1, gamma = gamma1,\n",
    "              learning_rate = lr1, num_trees = num_trees1)\n",
    "\n",
    "myGBDT = myGBDT.fit(X_train, y_train)\n",
    "train_pred = myGBDT.predict(X_train)\n",
    "test_pred = myGBDT.predict(X_test)\n",
    "train_acc = accuracy(train_pred,y_train)\n",
    "test_acc = accuracy(test_pred,y_test)\n",
    "print(\"train_acc=\", train_acc)\n",
    "print(\"test_acc=\", test_acc)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "max_depth1=4\n",
    "min_sample_split1=47 \n",
    "lamda1=5.43\n",
    "gamma1=0.9\n",
    "rf1=0.4\n",
    "num_trees1=35\n",
    "myRF = RF( n_threads = None, loss = 'log',\n",
    "              max_depth = max_depth1, min_sample_split = min_sample_split1, \n",
    "              lamda = lamda1, gamma = gamma1, rf=rf1,\n",
    "              num_trees = num_trees1)\n",
    "\n",
    "myRF = myRF.fit(X_train, y_train)\n",
    "train_pred = myRF.predict(X_train)\n",
    "test_pred = myRF.predict(X_test)\n",
    "train_acc = accuracy(train_pred,y_train)\n",
    "test_acc = accuracy(test_pred,y_test)\n",
    "print(\"train_acc=\", train_acc)\n",
    "print(\"test_acc=\", test_acc)\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
